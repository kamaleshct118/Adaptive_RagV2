{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83c\udfe5 Offline Vector Indexing Pipeline (Medical Education)\n",
        "\n",
        "## \ud83c\udfaf Overview\n",
        "This notebook serves as the **Offline Preprocessing & Indexing Pipeline** for the Adaptive RAG system. Its sole purpose is to transform raw medical documents into a searchable vector index.\n",
        "\n",
        "### \ud83d\udeab Scope & Usage\n",
        "- **Offline Only**: This notebook is run *once* (or periodically) to build the database.\n",
        "- **No RAG/LLM**: It does not perform retrieval or generation.\n",
        "- **Zero Medical Advice**: It processes text data deterministically without interpretation.\n",
        "\n",
        "### \ud83d\udcc2 Output Artifacts\n",
        "This pipeline will generate the following files in a `./vector_store/` directory:\n",
        "1. `index.faiss`: The FAISS vector index for fast similarity search.\n",
        "2. `metadata.pkl`: Semantic metadata mapping for chunks.\n",
        "3. `texts.pkl`: The raw text content corresponding to vectors.\n",
        "4. `config.json`: Configuration used (embedding model name, chunk size) to ensure consistency.\n",
        "\n",
        "### \u26a0\ufe0f CRITICAL WARNING\n",
        "> **Consistency is Key**: The embedding model used here (`sentence-transformers/all-MiniLM-L6-v2`) **MUST** match the model used in the online query notebook. Mismatched models will result in random/garbage retrieval.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title \ud83d\udce6 Install Dependencies\n",
        "# We need specific libraries for OCR, PDF processing, and Vector Indexing.\n",
        "\n",
        "# 1. System dependencies for PDF handling and OCR\n",
        "!apt-get update -qq\n",
        "!apt-get install -y poppler-utils tesseract-ocr\n",
        "\n",
        "# 2. Python Libraries\n",
        "# sentence-transformers: For generating state-of-the-art text embeddings\n",
        "# faiss-cpu: Facebook AI Similarity Search (efficient vector storage)\n",
        "# pytesseract: Wrapper for Google's Tesseract-OCR\n",
        "# pdf2image: To convert PDF pages to images for OCR\n",
        "# opencv-python: For image preprocessing (noise removal)\n",
        "# tqdm: For progress bars\n",
        "!pip install -q sentence-transformers faiss-cpu pytesseract pdf2image opencv-python numpy tqdm\n",
        "\n",
        "print(\"\u2705 Libraries installed successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title \u2699\ufe0f Environment Setup\n",
        "import os\n",
        "import sys\n",
        "import shutil\n",
        "import logging\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# Create the output directory for our vector store\n",
        "OUTPUT_DIR = './vector_store'\n",
        "if os.path.exists(OUTPUT_DIR):\n",
        "    shutil.rmtree(OUTPUT_DIR) # Clean start\n",
        "os.makedirs(OUTPUT_DIR)\n",
        "print(f\"\ud83d\udcc2 Created output directory: {OUTPUT_DIR}\")\n",
        "\n",
        "# Tesseract Config (usually found automatically, but good to ensure availability)\n",
        "try:\n",
        "    import pytesseract\n",
        "    # Check if tesseract is in path\n",
        "    pytesseract.get_tesseract_version()\n",
        "    print(\"\u2705 Tesseract OCR is available.\")\n",
        "except Exception as e:\n",
        "    print(\"\u274c Tesseract OCR not found. Please verify installation.\")\n",
        "    raise e\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udd22 Step 1: Configuration\n",
        "\n",
        "We need to know how many documents you intend to process. We assign internal IDs to maintain lineage.\n",
        "Batch processing allows us to track progress and manage memory effectively.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Input Document Count\n",
        "try:\n",
        "    num_documents = int(input(\"Enter number of documents to process: \"))\n",
        "    print(f\"\ud83d\udccb We will process {num_documents} documents.\")\n",
        "except ValueError:\n",
        "    num_documents = 1\n",
        "    print(\"\u26a0\ufe0f Invalid input. Defaulting to 1 document.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title \ud83d\udce4 Upload Documents\n",
        "from google.colab import files\n",
        "\n",
        "print(f\"Please upload {num_documents} file(s) (PDF, JPG, PNG)...\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "source_files = list(uploaded.keys())\n",
        "\n",
        "if len(source_files) == 0:\n",
        "    raise ValueError(\"No files uploaded Exiting.\")\n",
        "\n",
        "print(\"\\nfiles to be processed:\")\n",
        "for i, f in enumerate(source_files):\n",
        "    print(f\"{i}: {f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title \ud83d\udd0d OCR Text Extraction\n",
        "from pdf2image import convert_from_path\n",
        "import pytesseract\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import io\n",
        "\n",
        "# Data structure to hold raw text\n",
        "documents = [] \n",
        "# Format: [{'doc_id': int, 'source': str, 'raw_text': str}]\n",
        "\n",
        "print(\"\ud83d\ude80 Starting OCR extraction... (This may take time via Tesseract)\")\n",
        "\n",
        "for doc_idx, filename in enumerate(source_files):\n",
        "    print(f\"\\n\ud83d\udcc4 Processing {filename} ({doc_idx+1}/{len(source_files)})...\")\n",
        "    \n",
        "    full_text = \"\"\n",
        "    file_ext = filename.split('.')[-1].lower()\n",
        "    \n",
        "    try:\n",
        "        if file_ext == 'pdf':\n",
        "            # Convert PDF to list of images\n",
        "            images = convert_from_path(filename)\n",
        "            \n",
        "            for i, image in enumerate(images):\n",
        "                # Convert to grayscale for better OCR\n",
        "                # Text extraction\n",
        "                text = pytesseract.image_to_string(image)\n",
        "                full_text += text + \"\\n\"\n",
        "                \n",
        "        elif file_ext in ['jpg', 'jpeg', 'png']:\n",
        "            image = Image.open(filename)\n",
        "            text = pytesseract.image_to_string(image)\n",
        "            full_text += text\n",
        "        else:\n",
        "            print(f\"\u26a0\ufe0f Skipping unsupported file type: {filename}\")\n",
        "            continue\n",
        "            \n",
        "        # Store result\n",
        "        documents.append({\n",
        "            \"doc_id\": doc_idx,\n",
        "            \"source\": filename,\n",
        "            \"raw_text\": full_text\n",
        "        })\n",
        "        print(f\"   \u2705 Extracted {len(full_text)} characters from {filename}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"   \u274c Error processing {filename}: {e}\")\n",
        "\n",
        "print(f\"\\n\ud83c\udfc1 OCR Complete. Processed {len(documents)} documents.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title \ud83e\uddf9 Noise Removal & Normalization\n",
        "import re\n",
        "import unicodedata\n",
        "\n",
        "def normalize_text(text):\n",
        "    # 1. Unicode normalization (NFKD to decompose special chars)\n",
        "    text = unicodedata.normalize('NFKD', text)\n",
        "    \n",
        "    # 2. Lowercase (Consistent for embeddings)\n",
        "    text = text.lower()\n",
        "    \n",
        "    # 3. Remove excess whitespace/newlines\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    \n",
        "    # 4. Remove common artifact patterns (e.g., page numbers like 'Page 1 of 5')\n",
        "    text = re.sub(r'page \\d+ of \\d+', '', text)\n",
        "    text = re.sub(r'page \\d+', '', text)\n",
        "    \n",
        "    return text\n",
        "\n",
        "print(\"Cleaning text...\\n\")\n",
        "for doc in documents:\n",
        "    original_len = len(doc['raw_text'])\n",
        "    doc['clean_text'] = normalize_text(doc['raw_text'])\n",
        "    cleaned_len = len(doc['clean_text'])\n",
        "    \n",
        "    print(f\"Doc {doc['doc_id']} ({doc['source']}): reduced {original_len} -> {cleaned_len} chars\")\n",
        "    \n",
        "# Note: This step is irreversible. We construct embeddings from valid semantic content only.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83e\udde9 Chunking Strategy\n",
        "\n",
        "We use a **sliding window** approach to chunking. \n",
        "\n",
        "- **Chunk Size**: Number of characters/tokens per chunk. Keeping this around 400-500 helps in capturing single concepts.\n",
        "- **Overlap**: Essential for medical text. Ensures that context (like a disease name appearing at the end of chunk A) is carried over to chunk B.\n",
        "- **Model Limit**: `all-MiniLM-L6-v2` works best with inputs under 256-512 tokens.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Chunking Parameters\n",
        "\n",
        "# Configurable parameters\n",
        "MAX_TOKENS = 500   # Not explicitly used if we chunk by char, but guides the design\n",
        "CHUNK_SIZE = 400   # Characters (approx 100 tokens)\n",
        "CHUNK_OVERLAP = 80 # Characters (approx 20 tokens)\n",
        "\n",
        "print(f\"Configuration: Size={CHUNK_SIZE}, Overlap={CHUNK_OVERLAP}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title \ud83d\udd2a Execute Chunking\n",
        "\n",
        "chunks = []\n",
        "chunk_counter = 0\n",
        "\n",
        "for doc in documents:\n",
        "    text = doc['clean_text']\n",
        "    source = doc['source']\n",
        "    doc_id = doc['doc_id']\n",
        "    \n",
        "    # Simple sliding window by character\n",
        "    # (For production, consider nltk sentence tokenizer or recursive chunking)\n",
        "    for i in range(0, len(text), CHUNK_SIZE - CHUNK_OVERLAP):\n",
        "        chunk_text = text[i : i + CHUNK_SIZE]\n",
        "        \n",
        "        # Skip chunks that are too small (noise)\n",
        "        if len(chunk_text) < 50:\n",
        "            continue\n",
        "            \n",
        "        chunks.append({\n",
        "            \"chunk_id\": chunk_counter,\n",
        "            \"doc_id\": doc_id,\n",
        "            \"text\": chunk_text,\n",
        "            \"source\": source,\n",
        "            \"position\": i\n",
        "        })\n",
        "        chunk_counter += 1\n",
        "\n",
        "print(f\"\u2705 Generated {len(chunks)} chunks from {len(documents)} documents.\")\n",
        "# Example peek\n",
        "if chunks:\n",
        "    print(\"Sample Chunk:\", chunks[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title \ud83e\udde0 Load Embedding Model\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "MODEL_NAME = 'sentence-transformers/all-MiniLM-L6-v2'\n",
        "\n",
        "print(f\"Loading model: {MODEL_NAME}...\")\n",
        "# This downloads the model weights\n",
        "embedding_model = SentenceTransformer(MODEL_NAME)\n",
        "\n",
        "dim = embedding_model.get_sentence_embedding_dimension()\n",
        "print(f\"\u2705 Model loaded. Embedding Dimension: {dim}\")\n",
        "print(\"\u26a0\ufe0f WARNING: If you change this model, you MUST re-run this entire notebook. Index varies by model!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title \u26a1 Generate Embeddings\n",
        "import numpy as np\n",
        "\n",
        "# Extract text list\n",
        "chunk_texts = [c['text'] for c in chunks]\n",
        "\n",
        "print(f\"Encoding {len(chunk_texts)} chunks... (Using CPU/GPU)\")\n",
        "\n",
        "# Generate embeddings\n",
        "# show_progress_bar=True provides a tqdm bar automatically\n",
        "embeddings = embedding_model.encode(chunk_texts, show_progress_bar=True, convert_to_numpy=True)\n",
        "\n",
        "# Ensure float32 for FAISS\n",
        "embeddings = embeddings.astype(np.float32)\n",
        "\n",
        "print(f\"\u2705 Embeddings shape: {embeddings.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title \ud83d\uddc4\ufe0f Create FAISS Index\n",
        "import faiss\n",
        "\n",
        "dimension = embeddings.shape[1]\n",
        "\n",
        "# Create L2 Index (Euclidean Distance). \n",
        "# Since embeddings are often normalized, L2 is proportional to Cosine Similarity.\n",
        "# For exact cosine similarity, we would normalize vectors first then use IndexFlatIP (Inner Product).\n",
        "# Here we stick to Standard L2 for robust testing.\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "\n",
        "# Add vectors to index\n",
        "index.add(embeddings)\n",
        "\n",
        "print(f\"\u2705 FAISS Index created. Total vectors: {index.ntotal}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title \ud83c\udf92 Prepare Metadata Store\n",
        "\n",
        "# FAISS only stores vectors. It doesn't know what text belongs to which vector.\n",
        "# We need a 'Sidecar' storage: ID -> Data mapping.\n",
        "\n",
        "metadata_store = {}\n",
        "text_store = {}\n",
        "\n",
        "for i, chunk in enumerate(chunks):\n",
        "    # chunk['chunk_id'] corresponds to the index in FAISS (sequential 0..N)\n",
        "    # In this simple case, index ID == chunk_id because we added them in order.\n",
        "    \n",
        "    c_id = chunk['chunk_id']\n",
        "    \n",
        "    # Metadata: Source info\n",
        "    metadata_store[c_id] = {\n",
        "        \"doc_id\": chunk['doc_id'],\n",
        "        \"source\": chunk['source'],\n",
        "        \"position\": chunk['position']\n",
        "    }\n",
        "    \n",
        "    # Text: The actual content\n",
        "    text_store[c_id] = chunk['text']\n",
        "\n",
        "print(f\"\u2705 Prepared metadata for {len(metadata_store)} items.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title \ud83d\udcbe Save to Disk\n",
        "import pickle\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "# Paths\n",
        "index_path = os.path.join(OUTPUT_DIR, 'index.faiss')\n",
        "metadata_path = os.path.join(OUTPUT_DIR, 'metadata.pkl')\n",
        "texts_path = os.path.join(OUTPUT_DIR, 'texts.pkl')\n",
        "config_path = os.path.join(OUTPUT_DIR, 'config.json')\n",
        "\n",
        "# 1. Save FAISS Index\n",
        "faiss.write_index(index, index_path)\n",
        "\n",
        "# 2. Save Metadata (Pickle)\n",
        "with open(metadata_path, 'wb') as f:\n",
        "    pickle.dump(metadata_store, f)\n",
        "    \n",
        "# 3. Save Texts (Pickle)\n",
        "with open(texts_path, 'wb') as f:\n",
        "    pickle.dump(text_store, f)\n",
        "\n",
        "# 4. Save Config (JSON) for reproducibility\n",
        "config_data = {\n",
        "    \"embedding_model\": MODEL_NAME,\n",
        "    \"chunk_size\": CHUNK_SIZE,\n",
        "    \"chunk_overlap\": CHUNK_OVERLAP,\n",
        "    \"num_documents\": len(documents),\n",
        "    \"total_chunks\": len(chunks),\n",
        "    \"timestamp\": str(datetime.now())\n",
        "}\n",
        "with open(config_path, 'w') as f:\n",
        "    json.dump(config_data, f, indent=4)\n",
        "\n",
        "print(\"\ud83d\udcbe All artifacts saved successfully to ./vector_store/:\")\n",
        "print(f\"  - {index_path}\")\n",
        "print(f\"  - {metadata_path}\")\n",
        "print(f\"  - {texts_path}\")\n",
        "print(f\"  - {config_path}\")\n",
        "\n",
        "# Create a zip for easy download\n",
        "shutil.make_archive('vector_store_backup', 'zip', OUTPUT_DIR)\n",
        "print(\"\\n\ud83d\udce6 Created 'vector_store_backup.zip' for download.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83c\udfc1 Pipeline Complete\n",
        "\n",
        "**Summary:**\n",
        "We have successfully converted your raw documents into a searchable Vector Database.\n",
        "\n",
        "**Artifacts Created:**\n",
        "1. `index.faiss`: Geometry of your data.\n",
        "2. `metadata.pkl`: Links vectors to document sources.\n",
        "3. `texts.pkl`: The read-able text returned to the LLM.\n",
        "\n",
        "**Next Steps:**\n",
        "- Download `vector_store_backup.zip`.\n",
        "- Upload it to your **Online RAG Notebook**.\n",
        "- Load the `all-MiniLM-L6-v2` model there to query this data.\n",
        "\n",
        "> **Note:** If you add new documents later, you must re-run this entire pipeline to regenerate the index.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}