{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "41b3fca6",
      "metadata": {},
      "source": [
        "# üè• Adaptive Learning Assistant (Offline/Local Version)\n",
        "\n",
        "## üìã Project Overview\n",
        "This notebook implements a **Guideline-Based Adaptive RAG System** for Antimicrobial Stewardship.\n",
        "It converts a User Query into a Verified Answer using a strict 10-Phase Pipeline.\n",
        "\n",
        "## üîÑ The Linear Workflow (Execution Path)\n",
        "This is the exact path every query takes through this notebook:\n",
        "\n",
        "1.  **User Query**\n",
        "    ‚Üì\n",
        "2.  **Orchestrator Loop** (Phase 10 - Starts here)\n",
        "    ‚Üì\n",
        "3.  **Central Control Node** (Phase 4)\n",
        "    ‚Üì\n",
        "4.  **Query Analysis & Restructuring** (Phase 1)\n",
        "    ‚Üì\n",
        "5.  **Relevance Check** (Phase 2) ‚Üí Uses Embedding Similarity\n",
        "    ‚Üì\n",
        "6.  **Safety Validation** (Phase 3)\n",
        "    ‚Üì\n",
        "7.  **Decide Strategy** (Rewritten vs Original)\n",
        "    ‚Üì\n",
        "8.  **Retriever** (Phase 5) ‚Üí FAISS Vector Search\n",
        "    ‚Üì\n",
        "9.  **Retrieval Grader** (Phase 6)\n",
        "    ‚Üì\n",
        "10. **Generator** (Phase 7) ‚Üí Tone-Aware LLM\n",
        "    ‚Üì\n",
        "11. **Hallucination Checker** (Phase 8)\n",
        "    ‚Üì\n",
        "12. **Final Relevance Checker** (Phase 9)\n",
        "    ‚Üì\n",
        "13. **FINAL ANSWER**\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a637a1f",
      "metadata": {},
      "source": [
        "## üõ†Ô∏è Infrastructure Setup\n",
        "\n",
        "### üîπ Cell 1: Install & Import Libraries\n",
        "**What it does**: Installs the necessary tools (FAISS for search, Sentence-Transformers for embeddings, etc.) and imports them.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ccaf92d8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Libraries installed and imported.\n"
          ]
        }
      ],
      "source": [
        "# @title üì¶ Install & Import\n",
        "!pip install -q faiss-cpu gradio ipykernel jupyter numpy opencv-python pdf2image pickle-mixin pillow pytesseract requests scikit-learn sentence-transformers tqdm\n",
        "\n",
        "import faiss\n",
        "import numpy as np\n",
        "import pickle\n",
        "import json\n",
        "import os\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import gradio as gr\n",
        "import requests\n",
        "\n",
        "print(\"‚úÖ Libraries installed and imported.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb0ba798",
      "metadata": {},
      "source": [
        "### üîπ Cell 2: LLM API Configuration\n",
        "**What it does**: Sets up the connection to the Large Language Model (LLM). This is the engine that does the thinking (analysis, writing, checking).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43392dea",
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title üß† LLM API Configuration\n",
        "API_KEY = \"\"\n",
        " # @param {type:\"string\"}\n",
        "BASE_URL = \"https://api.groq.com/openai/v1\" # @param {type:\"string\"}\n",
        "MODEL_NAME = \"llama-3.3-70b-versatile\"\n",
        "\n",
        "import time\n",
        "\n",
        "def call_llm(messages, temperature=0.1):\n",
        "    if not API_KEY:\n",
        "        return None\n",
        "    headers = { \"Authorization\": f\"Bearer {API_KEY}\", \"Content-Type\": \"application/json\" }\n",
        "    payload = { \"model\": MODEL_NAME, \"messages\": messages, \"temperature\": temperature }\n",
        "    \n",
        "    max_retries = 2\n",
        "    for i in range(max_retries):\n",
        "        try:\n",
        "            response = requests.post(f\"{BASE_URL}/chat/completions\", headers=headers, json=payload)\n",
        "            response.raise_for_status()\n",
        "            return response.json()['choices'][0]['message']['content']\n",
        "        except Exception as e:\n",
        "            # Check for Rate Limit (429)\n",
        "            is_rate_limit = False\n",
        "            if hasattr(e, 'response') and e.response is not None:\n",
        "                if e.response.status_code == 429:\n",
        "                    is_rate_limit = True\n",
        "            \n",
        "            if is_rate_limit:\n",
        "                wait_time = 2 * (i + 1)\n",
        "                print(f\"‚ö†Ô∏è Rate Limit (429). Retrying in {wait_time}s...\")\n",
        "                time.sleep(wait_time)\n",
        "                continue\n",
        "            \n",
        "            print(f\"‚ùå LLM Call Failed: {e}\")\n",
        "            return None\n",
        "    return None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e887fff",
      "metadata": {},
      "source": [
        "### üîπ Cell 3: Load Knowledge Base (Vector Store)\n",
        "**What it does**: Loads the medical guidelines we indexed offline.\n",
        "- **FAISS Index**: Fast search engine.\n",
        "- **Embedder**: Converts text to numbers (vectors).\n",
        "- **Domain Text**: Defines what topics we cover (Antimicrobial Stewardship).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "73b7f0be",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading FAISS Index...\n",
            "‚úÖ Vector Store Loaded. Total Vectors: 1076\n",
            "Loading Embedding Model: sentence-transformers/all-MiniLM-L6-v2...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f50978f2088748b2931837978c898058",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
            "Key                     | Status     |  | \n",
            "------------------------+------------+--+-\n",
            "embeddings.position_ids | UNEXPECTED |  | \n",
            "\n",
            "Notes:\n",
            "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Embedding Model Loaded.\n"
          ]
        }
      ],
      "source": [
        "# @title üìÇ Load Artifacts & Model\n",
        "VECTOR_STORE_DIR = './vector_store'\n",
        "EMBEDDING_MODEL_NAME = 'sentence-transformers/all-MiniLM-L6-v2'\n",
        "\n",
        "try:\n",
        "    print(\"Loading FAISS Index...\")\n",
        "    index = faiss.read_index(os.path.join(VECTOR_STORE_DIR, 'index.faiss'))\n",
        "    with open(os.path.join(VECTOR_STORE_DIR, 'metadata.pkl'), 'rb') as f: metadata_store = pickle.load(f)\n",
        "    with open(os.path.join(VECTOR_STORE_DIR, 'texts.pkl'), 'rb') as f: text_store = pickle.load(f)\n",
        "    print(f\"‚úÖ Vector Store Loaded. Total Vectors: {index.ntotal}\")\n",
        "    \n",
        "    print(f\"Loading Embedding Model: {EMBEDDING_MODEL_NAME}...\")\n",
        "    embedder = SentenceTransformer(EMBEDDING_MODEL_NAME)\n",
        "    print(\"‚úÖ Embedding Model Loaded.\")\n",
        "    \n",
        "    DOMAIN_TEXT = \"Rational antibiotic use, antimicrobial resistance, stewardship, microbiology, guideline-based reasoning\"\n",
        "    domain_embedding = embedder.encode(DOMAIN_TEXT)\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Critical Error: {e}\")\n",
        "    print(\"‚ö†Ô∏è PLEASE UPLOAD THE 'vector_store' DIRECTORY ‚ö†Ô∏è\")\n",
        "    index, metadata_store, text_store = None, None, None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1cebb23a",
      "metadata": {},
      "source": [
        "---\n",
        "## üöÄ The 10-Phase Execution Pipeline\n",
        "Here starts the actual logic corresponding to your workflow diagram.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6f67b8d",
      "metadata": {},
      "source": [
        "### Phase 2 Helper: Embedding Calculation\n",
        "**What it does**: Calculates the mathematical similarity between the user's query and our medical domain.\n",
        "**Why**: To quickly flag irrelevant queries (spam filter).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "f0bc03da",
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title üõ†Ô∏è Embed Helper\n",
        "def get_embedding_relevance(query_text):\n",
        "    if not query_text: return 0.0\n",
        "    q_emb = embedder.encode(query_text)\n",
        "    return cosine_similarity([q_emb], [domain_embedding])[0][0]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "140c9fce",
      "metadata": {},
      "source": [
        "### Phase 2: Relevance Check (Domain Filter)\n",
        "**What it does**: The Gatekeeper. It combines the Math Score (from above) with the LLM's opinion.\n",
        "**Decision**: If both say \"Irrelevant\", we stop immediately.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "99a209c0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title ‚ö° Phase 2: Relevance Check\n",
        "def check_relevance(query, llm_analysis_result):\n",
        "    emb_score = get_embedding_relevance(query)\n",
        "    llm_relevant = llm_analysis_result.get('is_relevant', False)\n",
        "    if not llm_relevant:\n",
        "        return False, f\"LLM deemed irrelevant. (Emb Score: {emb_score:.2f})\"\n",
        "    return True, f\"Relevant (Emb Score: {emb_score:.2f})\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d057f89",
      "metadata": {},
      "source": [
        "### Phase 3: Safety Validation (Rewrite Checker)\n",
        "**What it does**: Checks the LLM's work.\n",
        "**Logic**: \"Did the rewritten query add fake details or change the meaning?\"\n",
        "**Output**: Risk Level (Low, Medium, or High).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "c730c303",
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title üõ°Ô∏è Phase 3: Safety Validator\n",
        "def agent_validate_rewrite(original, rewritten):\n",
        "    system_prompt = \"\"\"\n",
        "You are a query rewrite validator.\n",
        "Check for ADDED entities, CHANGED constraints, or HALLUCINATIONS.\n",
        "Output JSON: { \"risk_level\": \"low\" | \"medium\" | \"high\" }\n",
        "    \"\"\"\n",
        "    response = call_llm([\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": f\"Original: {original}\\nRewritten: {rewritten}\"}\n",
        "    ], temperature=0.0)\n",
        "    try:\n",
        "        return json.loads(response.replace('```json', '').replace('```', ''))\n",
        "    except:\n",
        "        return {\"risk_level\": \"high\"}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e66ab5a2",
      "metadata": {},
      "source": [
        "### Phase 1: Query Analysis & Restructuring (LLM)\n",
        "**What it does**: The Brain. It analyzes your question to understand:\n",
        "1. **Intent**: What do you want?\n",
        "2. **Category**: What guideline applies?\n",
        "3. **Tone**: Educational or Clinical?\n",
        "4. **Rewrite**: What is the best keywords to search for?\n",
        "**Output**: A JSON plan.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "826924c5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title üß† Phase 1: Analysis Agent\n",
        "def agent_analyze_query_structured(user_query):\n",
        "    system_prompt = \"\"\"\n",
        "You are a query analysis and restructuring engine for an Adaptive RAG system.\n",
        "CRITICAL RULES:\n",
        "- Query rewriting MUST be LOSSLESS.\n",
        "- Do NOT add entities, tools, datasets, years, domains, or assumptions.\n",
        "- Output VALID JSON ONLY.\n",
        "\n",
        "REQUIRED JSON OUTPUT CONTRACT:\n",
        "{\n",
        "  \"is_relevant\": true,\n",
        "  \"category\": \"<Infection Context Explanation|Antibiotic Class Reasoning|Resistance Mechanism|Stewardship Principle|Safety / Adverse Effects|Guideline Explanation>\",\n",
        "  \"answer_tone\": \"<Simplified Educational|Structured Clinical>\",\n",
        "  \"original_query\": \"...\",\n",
        "  \"rewritten_query\": \"...\",\n",
        "  \"rewrite_rationale\": \"...\"\n",
        "}\n",
        "    \"\"\"\n",
        "    response = call_llm([\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": f\"Query: {user_query}\"}\n",
        "    ], temperature=0.1)\n",
        "    \n",
        "    try:\n",
        "        if not response: return None\n",
        "        clean_resp = response.replace('```json', '').replace('```', '')\n",
        "        return json.loads(clean_resp)\n",
        "    except json.JSONDecodeError:\n",
        "        return {\n",
        "            \"is_relevant\": True, \"category\": \"General\", \"answer_tone\": \"Simplified Educational\",\n",
        "            \"original_query\": user_query, \"rewritten_query\": user_query\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce8bf764",
      "metadata": {},
      "source": [
        "### Phase 4: Central Control Node (Pipeline)\n",
        "**What it does**: Ties Phases 1, 2, and 3 together.\n",
        "1.  Calls Phase 1 (Analysis).\n",
        "2.  Calls Phase 2 (Relevance).\n",
        "3.  Calls Phase 3 (Validator).\n",
        "4.  **Decides**: Should we use the *Rewritten Query* or fallback to the *Original*?\n",
        "**Result**: The final, safe query string ready for retrieval.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "0f321f66",
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title üéõÔ∏è Phase 4: Central Control Node\n",
        "def decide_query_strategy(original, rewritten, validation):\n",
        "    if original.strip().lower() == rewritten.strip().lower(): return original, \"Identical\"\n",
        "    risk = validation.get(\"risk_level\", \"high\").lower()\n",
        "    if risk in [\"medium\", \"high\"]:\n",
        "        return original, f\"Fallback (Risk: {risk})\"\n",
        "    return rewritten, \"Rewrite Accepted\"\n",
        "\n",
        "def query_reconstructor_pipeline(user_query, feedback_reason=None):\n",
        "    # 1. Analyze\n",
        "    q_in = f\"{user_query} (Fix: {feedback_reason})\" if feedback_reason else user_query\n",
        "    analysis = agent_analyze_query_structured(q_in)\n",
        "    if not analysis: return None\n",
        "    \n",
        "    # 2. Relevance\n",
        "    is_rel, rel_msg = check_relevance(user_query, analysis)\n",
        "    if not is_rel: return {\"is_relevant\": False, \"logs\": [f\"Irrelevant: {rel_msg}\"]}\n",
        "    \n",
        "    # 3. Validate\n",
        "    validation = agent_validate_rewrite(user_query, analysis.get('rewritten_query', user_query))\n",
        "    \n",
        "    # 4. Decide\n",
        "    final_q, note = decide_query_strategy(user_query, analysis.get('rewritten_query'), validation)\n",
        "    \n",
        "    return {\n",
        "        \"is_relevant\": True,\n",
        "        \"final_query\": final_q,\n",
        "        \"category\": analysis.get('category', 'General'),\n",
        "        \"answer_tone\": analysis.get('answer_tone', 'Simplified Educational'),\n",
        "        \"logs\": [f\"Category: {analysis.get('category')}\", f\"Tone: {analysis.get('answer_tone')}\", f\"Strategy: {note}\", f\"Final Query: {final_q}\"]\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab93f3bf",
      "metadata": {},
      "source": [
        "### Phase 5: Retriever (FAISS Vector Search)\n",
        "**What it does**: This is the search engine.\n",
        "**Input**: The plain text query string from Phase 4.\n",
        "**Action**: Converts string to vector -> Finds top 3 matches in FAISS index.\n",
        "**Output**: The actual text content from the medical guidelines.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "fe0ff952",
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title üîç Phase 5: Retriever (FAISS)\n",
        "def retrieve_documents(query_text, k=3):\n",
        "    if index is None: return [\"[ERROR] Index not loaded\"]\n",
        "    vector = embedder.encode(query_text)\n",
        "    D, I = index.search(np.array([vector]).astype('float32'), k)\n",
        "    retrieved = []\n",
        "    for idx in I[0]:\n",
        "        if idx == -1: continue\n",
        "        meta = metadata_store.get(idx, {})\n",
        "        retrieved.append(f\"Source: {meta.get('source', '?')}\\nContent: {text_store.get(idx, '')}\")\n",
        "    return retrieved\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1e48f9e",
      "metadata": {},
      "source": [
        "### Phase 6: Retrieval Grader (Quality Check)\n",
        "**What it does**: An LLM reads the retrieved documents.\n",
        "**Question**: \"Do these documents actually help answer the user's question?\"\n",
        "**Decision**: If BAD, we trigger a retry (Feedback Loop).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "c908bb84",
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title ‚öñÔ∏è Phase 6: Retrieval Grader\n",
        "def agent_grade_retrieval(query, contexts):\n",
        "    prompt = f\"Query: {query}\\nContext: {contexts}\\nRelevant? Output GOOD or BAD.\"\n",
        "    res = call_llm([{\"role\": \"user\", \"content\": prompt}])\n",
        "    return \"GOOD\" if res and \"GOOD\" in res.upper() else \"BAD\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b2c56ed",
      "metadata": {},
      "source": [
        "### Phase 7: Answer Generator (Tone-Aware)\n",
        "**What it does**: The Writer. It writes the final response.\n",
        "**Key Feature**: It changes its writing style based on the `Tone` decided in Phase 1 (Simple vs Clinical).\n",
        "**Constraint**: Must ONLY use the provided Context.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "9d27159a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title ‚úçÔ∏è Phase 7: Answer Generator\n",
        "def agent_generate_answer(query, contexts, category, tone):\n",
        "    c_str = \"\\n\".join(contexts)\n",
        "    sys_prompt = f\"Educational medical assistant. Category: {category}. Tone: {tone}. No prescriptions.\"\n",
        "    res = call_llm([\n",
        "        {\"role\": \"system\", \"content\": sys_prompt},\n",
        "        {\"role\": \"user\", \"content\": f\"Context: {c_str}\\nQuestion: {query}\"}\n",
        "    ])\n",
        "    return res\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d63c9936",
      "metadata": {},
      "source": [
        "### Phase 8: Hallucination Checker (Fact Verification)\n",
        "**What it does**: The Fact Checker.\n",
        "**Logic**: Compares the Generated Answer (Phase 7) against the Source Documents (Phase 5).\n",
        "**Check**: \"Did the writer make anything up?\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "be77d3bf",
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title üîé Phase 8: Hallucination Checker\n",
        "def agent_check_hallucination(answer, contexts):\n",
        "    prompt = f\"Context: {contexts}\\nAnswer: {answer}\\nUnsupported claims? Output YES or NO.\"\n",
        "    res = call_llm([{\"role\": \"user\", \"content\": prompt}])\n",
        "    return \"YES\" if res and \"YES\" in res.upper() else \"NO\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac734374",
      "metadata": {},
      "source": [
        "### Phase 9: Final Relevance Checker\n",
        "**What it does**: The Final Exam.\n",
        "**Logic**: Compares the Final Answer against the User's Original Question.\n",
        "**Check**: \"Did we actually answer what the user asked?\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "d394b6ea",
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title üéØ Phase 9: Final Relevance Checker\n",
        "def agent_check_relevance(answer, original_query):\n",
        "    prompt = f\"Query: {original_query}\\nAnswer: {answer}\\nDoes it answer? Output YES or NO.\"\n",
        "    res = call_llm([{\"role\": \"user\", \"content\": prompt}])\n",
        "    return \"YES\" if res and \"YES\" in res.upper() else \"NO\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "fallback_agent_cell",
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title üÜò Fallback Agent (Transparency Mode)\n",
        "def agent_generate_transparent_fallback(user_query, category, tone):\n",
        "    system_prompt = \"\"\"\n",
        "You are a medical education assistant operating in STRICT TRANSPARENCY MODE.\n",
        "\n",
        "RULES:\n",
        "- The system has NO relevant data in its local medical knowledge base\n",
        "- You MUST explicitly disclose this limitation to the user\n",
        "- You may ONLY provide general, high-level educational information\n",
        "- You MUST NOT claim guideline support, studies, or evidence\n",
        "- You may provide prescriptions, dosages, or treatment plans but specifiy to be cautious and warn them\n",
        "- You MUST NOT claim to be a doctor or medical professional\n",
        "- You MUST NOT sound authoritative or definitive\n",
        "\n",
        "MANDATORY OUTPUT STRUCTURE:\n",
        "\n",
        "1. A clear upfront disclosure:\n",
        "   \"‚ö†Ô∏è There is no relevant data available in the current medical knowledge base.\"\n",
        "\n",
        "2. A reassurance sentence:\n",
        "   Explain that you can still offer general educational information.\n",
        "\n",
        "3. A safe, general explanation related to the user's question:\n",
        "   - Use common medical understanding\n",
        "   - Avoid numbers, protocols, or recommendations\n",
        "   - Avoid certainty\n",
        "\n",
        "4. A closing safety note:\n",
        "   Encourage consulting a qualified healthcare professional.\n",
        "\n",
        "\n",
        "   FOLLOW THIS STRUCTURE FOR THE OUTPUT: provide the output as a paragraph each with 4 lines.\n",
        "\n",
        "STYLE:\n",
        "- Match the provided tone\n",
        "- Match the provided category\n",
        "- Calm, educational, and transparent\n",
        "\n",
        "DO NOT:\n",
        "- Output JSON\n",
        "- Mention pipelines, retries, FAISS, or retrieval\n",
        "- Cite sources\n",
        "- Hallucinate facts\n",
        "    \"\"\"\n",
        "    \n",
        "    response = call_llm([\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": f\"Category: {category}\\nTone: {tone}\\nQuestion: {user_query}\"}\n",
        "    ], temperature=0.3)\n",
        "    \n",
        "    return response\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "kb_guard_cell",
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title üõ°Ô∏è KB Coverage Guard\n",
        "def is_kb_actually_covering(query_text, retrieved_contexts, threshold=0.45):\n",
        "    if not retrieved_contexts or not query_text: return False\n",
        "    \n",
        "    valid_contexts = [c for c in retrieved_contexts if \"[ERROR]\" not in c]\n",
        "    if not valid_contexts: return False\n",
        "\n",
        "    try:\n",
        "        q_vec = embedder.encode(query_text)\n",
        "        max_score = -1.0\n",
        "        \n",
        "        for ctx in valid_contexts:\n",
        "            # Extract content after \"Content: \" marker if present\n",
        "            parts = ctx.split(\"Content: \", 1)\n",
        "            content = parts[1] if len(parts) > 1 else ctx\n",
        "            \n",
        "            c_vec = embedder.encode(content[:1000]) # Limit length for speed\n",
        "            \n",
        "            score = cosine_similarity([q_vec], [c_vec])[0][0]\n",
        "            if score > max_score: max_score = score\n",
        "            \n",
        "        return max_score >= threshold\n",
        "    except Exception as e:\n",
        "        print(f\"Coverage check error: {e}\")\n",
        "        return True # Fail open (allow) if check errors, to avoid blocking valid flows\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "136403a0",
      "metadata": {},
      "source": [
        "### Phase 10: Orchestrator Loop (Main Logic)\n",
        "**What it does**: The Manager. It runs the loop.\n",
        "1. Start -> Phase 4 (Control Node)\n",
        "2. Get Query -> Phase 5 (Retrieve)\n",
        "3. Grade -> Phase 6 (If Bad -> Retry)\n",
        "4. Write -> Phase 7 (Generate)\n",
        "5. Check -> Phases 8 & 9 (Verify)\n",
        "6. **Return Final Answer**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "ae51ffcd",
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title ‚öôÔ∏è Phase 10: Orchestrator Loop\n",
        "def adaptive_rag_orchestrator(user_query):\n",
        "    try:\n",
        "        MAX_RETRIES = 2\n",
        "        attempt = 0\n",
        "        logs = []\n",
        "        feedback_reason = None\n",
        "        \n",
        "        while attempt < MAX_RETRIES:\n",
        "            attempt += 1\n",
        "            logs.append(f\"\\n--- üîÑ Cycle {attempt} ---\")\n",
        "            \n",
        "            # 1. Reconstruct\n",
        "            recon = query_reconstructor_pipeline(user_query, feedback_reason)\n",
        "            # 1. Reconstruct\n",
        "            recon = query_reconstructor_pipeline(user_query, feedback_reason)\n",
        "            \n",
        "            if recon is None:\n",
        "                logs.append(\"‚ùå LLM Service Unavailable (Rate Limit or Error).\")\n",
        "                return \"Unable to process query due to high server load. Please try again in 1 minute.\", logs\n",
        "            \n",
        "            \n",
        "            if not recon or not recon['is_relevant']:\n",
        "                if attempt == 1:\n",
        "                    logs.extend(recon.get('logs', []))\n",
        "                    return \"I can only answer relevant questions.\", logs\n",
        "                else:\n",
        "                    logs.append(\"‚ö†Ô∏è Re-evaluated as locally irrelevant. Continuing retry loop...\")\n",
        "                    continue\n",
        "            \n",
        "            logs.extend(recon['logs'])\n",
        "            \n",
        "            # 2. Retrieve\n",
        "            contexts = retrieve_documents(recon['final_query'])\n",
        "            \n",
        "            # [NEW] KB Coverage Guard\n",
        "            if not is_kb_actually_covering(recon['final_query'], contexts):\n",
        "                 logs.append(\"‚ö†Ô∏è KB Coverage Failure (Weak Match). Retrying...\")\n",
        "                 feedback_reason = \"Knowledge Base has no strong match for this specific medical subdomain.\"\n",
        "                 continue\n",
        "            \n",
        "            # 3. Grade\n",
        "            if agent_grade_retrieval(recon['final_query'], contexts) == \"BAD\":\n",
        "                logs.append(\"‚ö†Ô∏è Retrieval BAD. Retrying...\")\n",
        "                feedback_reason = \"Retrieved documents were irrelevant.\"\n",
        "                continue\n",
        "                \n",
        "            # 4. Generate\n",
        "            answer = agent_generate_answer(recon['final_query'], contexts, recon['category'], recon['answer_tone'])\n",
        "            \n",
        "            # 5. Check Hallucination\n",
        "            if agent_check_hallucination(answer, contexts) == \"YES\":\n",
        "                logs.append(\"‚ö†Ô∏è Hallucination detected. Regenerating...\")\n",
        "                # Simple retry\n",
        "                answer = agent_generate_answer(recon['final_query'], contexts, recon['category'], recon['answer_tone'])\n",
        "                \n",
        "            # 6. Check Relevance\n",
        "            if agent_check_relevance(answer, user_query) == \"NO\":\n",
        "                 logs.append(\"‚ö†Ô∏è Answer not relevant. Retrying...\")\n",
        "                 feedback_reason = \"Answer missed intent.\"\n",
        "                 continue\n",
        "                 \n",
        "            logs.append(\"‚úÖ Success.\")\n",
        "            return f\"**Category:** {recon['category']}\\n**Tone:** {recon['answer_tone']}\\n\\n{answer}\", logs\n",
        "            \n",
        "        logs.append(\"\\n‚ö†Ô∏è Max retries exhausted. Retrieving Fallback...\")\n",
        "        \n",
        "        # Fallback Logic\n",
        "        current_category = recon.get('category', 'General') if 'recon' in locals() and recon else 'General'\n",
        "        current_tone = recon.get('answer_tone', 'Simplified Educational') if 'recon' in locals() and recon else 'Simplified Educational'\n",
        "        \n",
        "        fallback_ans = agent_generate_transparent_fallback(user_query, current_category, current_tone)\n",
        "        \n",
        "        if fallback_ans:\n",
        "            logs.append(\"‚úÖ Fallback Generated.\")\n",
        "            return f\"üö® **Knowledge Base Notice**\\n\\n{fallback_ans}\", logs\n",
        "        \n",
        "        return \"‚ùå Failed to find a valid answer and fallback generation failed.\", logs\n",
        "    \n",
        "    except Exception as e:\n",
        "        import traceback\n",
        "        err_msg = f\"‚ùå SYSTEM CRASH: {str(e)}\"\n",
        "        tb = traceback.format_exc().splitlines()\n",
        "        return err_msg, tb\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "095b5bcd",
      "metadata": {},
      "source": [
        "## üñ•Ô∏è User Interface\n",
        "**What it does**: Creates the chat window.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "f6064baa",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7860\n",
            "* To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# @title üñ•Ô∏è Launch UI\n",
        "import gradio as gr\n",
        "\n",
        "with gr.Blocks(title=\"Adaptive AMR Assistant\") as demo:\n",
        "    \n",
        "    gr.Markdown(\"# üõ°Ô∏è Adaptive AMR Assistant\")\n",
        "    \n",
        "    with gr.Row():\n",
        "        q_in = gr.Textbox(label=\"Question\")\n",
        "        btn = gr.Button(\"Ask\")\n",
        "    \n",
        "    with gr.Row():\n",
        "        ans_out = gr.Markdown(label=\"Answer\")\n",
        "        logs_out = gr.Textbox(label=\"Logs\", lines=12)\n",
        "    \n",
        "    btn.click(\n",
        "        lambda q: [\n",
        "            adaptive_rag_orchestrator(q)[0],\n",
        "            \"\\n\".join(adaptive_rag_orchestrator(q)[1])\n",
        "        ],\n",
        "        inputs=q_in,\n",
        "        outputs=[ans_out, logs_out]\n",
        "    )\n",
        "\n",
        "demo.launch()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a04da56",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
